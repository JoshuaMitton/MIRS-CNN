{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the functionality to build, train, and test a CNN for predicting mosquito age, grouped age, species, and status.\n",
    "\n",
    "## Structure:\n",
    "* Import packages to be used.\n",
    "* Load mosquito data.\n",
    "* Define fucntions for plotting, visualisation, and logging.\n",
    "* Define a function to build the CNN.\n",
    "* Define a function to train the CNN.\n",
    "* Main section to organise data, define the CNN, and call the building and training of the CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import random as rn\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib\n",
    "\n",
    "font = {'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, metrics\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.regularizers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# rand_seed = np.random.randint(low=0, high=100)\n",
    "rand_seed = 16\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "## The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "## The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "## Force TensorFlow to use single thread.\n",
    "## Multiple threads are a potential source of\n",
    "## non-reproducible results.\n",
    "## For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}, intra_op_parallelism_threads=4) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "## The below tf.set_random_seed() will make random number generation\n",
    "## in the TensorFlow backend have a well-defined initial state.\n",
    "## For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data file is created using Loco Mosquito:\n",
    "https://github.com/magonji/MIMI-project/blob/master/Loco%20mosquito%204.0.ipynb\n",
    "\n",
    "### The data file has headings: Species - Status - RearCnd - Age - Country- Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_class():\n",
    "    def __init__(self, valid_perc):\n",
    "        df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/MIMI-Analysis/Data/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "        df.head(10)\n",
    "\n",
    "        df['AgeGroup'] = 0\n",
    "        df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "        df_vf = df[df['RearCnd']=='VF']\n",
    "        df_vf = df_vf[df_vf['Status']=='UN']\n",
    "        df = df[df['RearCnd']!='VF']\n",
    "        df = df[df['Status']!='UN']\n",
    "        df_l = df[df['RearCnd']=='TL']\n",
    "        df_l_g = df_l[df_l['Country']=='S']\n",
    "        \n",
    "        df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "        age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "        df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "        age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "        df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "        age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "        df_l_t = df_l[df_l['Country']=='T']\n",
    "        df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "        age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "        df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "        age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "        df_l_b = df_l[df_l['Country']=='B']\n",
    "        df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "        age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "        df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "        age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "        df_f = df[df['RearCnd']=='TF']\n",
    "        df_f_t = df_f[df_f['Country']=='T']\n",
    "        df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "        # df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "        df_f_b = df_f[df_f['Country']=='B']\n",
    "        df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "        age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "        df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "        age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "        df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "        df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "        age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "        age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "        df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "        age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "        age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "\n",
    "        VF_size_t = len(df_vf_t)\n",
    "        VF_size_b = len(df_vf_b)\n",
    "#         print('validation size tanzania : {}'.format(VF_size_t))\n",
    "#         print('validation size bobo : {}'.format(VF_size_b))\n",
    "        val_group_size_t = int(((((valid_perc*VF_size_t)/2)/3))) #/2 (t/b) /3 (age groups)\n",
    "        val_group_size_b = int(((((valid_perc*VF_size_b)/2)/3)))\n",
    "#         print('validation size for testing tanzania : {}'.format(val_group_size_t))\n",
    "#         print('validation size for testing bobo : {}'.format(val_group_size_b))\n",
    "\n",
    "        size_inc = 400\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            if age == 0:\n",
    "                df_train = df_temp.iloc[index_df_temp_inc]\n",
    "        #         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 400\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 300\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 300\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = val_group_size_t\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Tanzania Arabiensis VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            if age == 0:\n",
    "                df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "            else:\n",
    "                df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Tanzania Gambie VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = val_group_size_b\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Bobo Gambie VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Bobo Colluzzi VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "        print('Percentage of field mosquitoes inc {} - Num mosquitoes {} / {}'.format(valid_perc*100, len(df_train[df_train['RearCnd']=='VF']), len(df_vf)))\n",
    "        print('Total number of mosquitoes in the Train set {}'.format(len(df_train)))\n",
    "        \n",
    "        X = df_train.iloc[:,6:-1]\n",
    "        y_age = df_train[\"Age\"]\n",
    "        y_age_groups = df_train[\"AgeGroup\"]\n",
    "        y_species = df_train[\"Species\"]\n",
    "        y_status = df_train[\"Status\"]\n",
    "#         print('shape of X : {}'.format(X.shape))\n",
    "#         print('shape of y age : {}'.format(y_age.shape))\n",
    "#         print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "#         print('shape of y species : {}'.format(y_species.shape))\n",
    "#         print('shape of y status : {}'.format(y_status.shape))\n",
    "        self.X = np.asarray(X)\n",
    "        y_age = np.asarray(y_age)\n",
    "        self.y_age_groups = np.asarray(y_age_groups)\n",
    "        self.y_species = np.asarray(y_species)\n",
    "        y_status = np.asarray(y_status)\n",
    "\n",
    "        X_vf = df_test.iloc[:,6:-1]\n",
    "        y_age_vf = df_test[\"Age\"]\n",
    "        y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "        y_species_vf = df_test[\"Species\"]\n",
    "        y_status_vf = df_test[\"Status\"]\n",
    "#         print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "#         print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "#         print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "#         print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "#         print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "        self.X_vf = np.asarray(X_vf)\n",
    "        y_age_vf = np.asarray(y_age_vf)\n",
    "        self.y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "        self.y_species_vf = np.asarray(y_species_vf)\n",
    "        y_status_vf = np.asarray(y_status_vf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(valid_perc):\n",
    "    return data_loader_class(valid_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to create a new folder for the CNN outputs.\n",
    "Useful to stop forgetting to name a new folder when trying out a new model varient and overwriting a days training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folder(fold, to_build = False):\n",
    "    if not os.path.isdir(fold):\n",
    "        if to_build == True:\n",
    "            os.mkdir(fold)\n",
    "        else:\n",
    "            print('Directory does not exists, not creating directory!')\n",
    "    else:\n",
    "        if to_build == True:\n",
    "            raise NameError('Directory already exists, cannot be created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting confusion matrcies\n",
    "This normalizes the confusion matrix and ensures neat plotting for all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, output, save_path, model_name, fold,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          printout=False):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printout:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if printout:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printout:\n",
    "        print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1) # np.max(np.sum(cm, axis=1)))\n",
    "#     plt.title([title+' - '+model_name])\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "#     plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig((save_path+\"Confusion_Matrix_\"+model_name+\"_\"+fold+\"_\"+output[1:]+\".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for visualizing outputs\n",
    "This splits the output data into the four categories before plotting the confusion matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for visualizing losses and metrics once the neural network fold is trained\n",
    "def visualize(histories, save_path, model_name, fold, classes, outputs, predicted, true):\n",
    "    # Sort out predictions and true labels\n",
    "    for label_predictions_arr, label_true_arr, classes, outputs in zip(predicted, true, classes, outputs):\n",
    "        classes_pred = np.argmax(label_predictions_arr, axis=-1)\n",
    "        classes_true = np.argmax(label_true_arr, axis=-1)\n",
    "        cnf_matrix = confusion_matrix(classes_true, classes_pred)\n",
    "        plot_confusion_matrix(cnf_matrix, classes, outputs, save_path, model_name, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for logging data associated with the model\n",
    "def log_data(log, name, fold, save_path):\n",
    "    f = open((save_path+name+'_'+str(fold)+'_log.txt'), 'w')\n",
    "    np.savetxt(f, log)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name, savedir_main):\n",
    "    with open(savedir_main + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name, savedir_main):\n",
    "    with open(savedir_main + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion for graphing the training data\n",
    "This fucntion creates tidy graphs of loss and accuracy as the models are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_history(history, model_name, model_ver_num, fold, save_path):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    #not_validation = list(filter(lambda x: x[0:3] != \"val\", history.history.keys()))\n",
    "#     print('history.history.keys : {}'.format(history.history.keys()))\n",
    "    filtered = filter(lambda x: x[0:3] != \"val\", history.history.keys())\n",
    "    not_validation = list(filtered)\n",
    "    for i in not_validation:\n",
    "        plt.figure(figsize=(15,7))\n",
    "#         plt.title(i+\"/ \"+\"val_\"+i)\n",
    "        plt.plot(history.history[i], label=i)\n",
    "        plt.plot(history.history[\"val_\"+i], label=\"val_\"+i)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(i)\n",
    "        plt.savefig(save_path +model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layer):\n",
    "\n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "\n",
    "    for i, layerwidth in zip(range(len(model_shape)),model_shape):\n",
    "        if i == 0:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd) \n",
    "                \n",
    "        else:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                if model_shape[i-1]['type'] == 'c':\n",
    "                    xd = Flatten()(xd)\n",
    "                    \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd)\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "        \n",
    "    \n",
    "#     xAge     = Dense(name = 'age', units = 17, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "    xSpecies = Dense(name ='species', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup', 'xSpecies']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "    print(model.metrics)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "    \n",
    "    out_path = save_path+'out/'\n",
    "    build_folder(out_path, True)\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "#     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    label = model_to_test[\"labels\"][0]\n",
    "    features = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "#     ## Split into training / testing\n",
    "#     test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "#     ## Pack up data\n",
    "#     X_train = test_splits.pop(0)\n",
    "#     X_val = test_splits.pop(0)\n",
    "#     y_train = test_splits[::2]\n",
    "#     y_val = test_splits[1::2]\n",
    "    \n",
    "#     out_model = create_models(model_shape, input_layer_dim, SelectFreqs)\n",
    "#     out_model.summary()\n",
    "#     out_history = out_model.fit(x = X_train, \n",
    "#                             y = y_train,\n",
    "#                             batch_size = 128*16, \n",
    "#                             verbose = 0, \n",
    "#                             epochs = 8000,\n",
    "#                             validation_data = (X_val, y_val),\n",
    "#                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                         patience=400, verbose=0, mode='auto'), \n",
    "#                                         CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "#     scores = out_model.evaluate(X_val, y_val)\n",
    "#     print(out_model.metrics_names)\n",
    "\n",
    "    ## Kfold training\n",
    "    seed = rand_seed\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    ## Split data into test and train\n",
    "    \n",
    "    model_ver_num = 0\n",
    "    cv_scores = []\n",
    "    best_score = 0\n",
    "    for train_index, val_index in kfold.split(features):\n",
    "        print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "        X_train, X_val = features[train_index], features[val_index]\n",
    "        y_train, y_val = list(map(lambda y:y[train_index], label)), list(map(lambda y:y[val_index], label))\n",
    "\n",
    "        model = create_models(model_shape, input_layer_dim)\n",
    "        if model_ver_num == 0:\n",
    "            model.summary()\n",
    "\n",
    "        history = model.fit(x = X_train, \n",
    "                            y = y_train,\n",
    "                            batch_size = 128*16, \n",
    "                            verbose = 0, \n",
    "                            epochs = 8000,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        patience=400, verbose=0, mode='auto'), \n",
    "                                        CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "        scores = model.evaluate(X_val, y_val)\n",
    "        model.save((out_path+model_name+\"_\"+str(model_ver_num)+'_Model.h5'))\n",
    "        graph_history(history, model_name, model_ver_num, 0, out_path)\n",
    "#         print(model.metrics_names)\n",
    "#         print(scores)\n",
    "        if (scores[3] + scores[4]) > best_score:\n",
    "            out_model = model\n",
    "            out_history = history\n",
    "        \n",
    "        model_ver_num += 1\n",
    "        \n",
    "#         # Clear the Keras session, otherwise it will keep adding new\n",
    "#         # models to the same TensorFlow graph each time we create\n",
    "#         # a model with a different set of hyper-parameters.\n",
    "#         K.clear_session()\n",
    "\n",
    "#         # Delete the Keras model with these hyper-parameters from memory.\n",
    "#         del model\n",
    "        \n",
    "    out_model.save((save_path+'Best_Model.h5'))\n",
    "    graph_history(out_history, 'Best_Model', 0, 0, save_path)\n",
    "    \n",
    "    return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "\n",
    "Functionality:\n",
    "* Oganises the data into a format of lists of data, classes, labels.\n",
    "* Define the CNN to be built.\n",
    "* Define the KFold validation to be used.\n",
    "* Build a folder to output data into.\n",
    "* Standardize and oragnise data into training/testing.\n",
    "* Call the model training.\n",
    "* Organize outputs and call visualization for plotting and graphing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "Fold 0 Running\n",
      "['acc']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 403, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 401, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 401, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 401, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 198, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 198, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 99, 16)       0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1584)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout5 (Dropout)                 (None, 1584)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d5 (Dense)                      (None, 400)          634000      dout5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 400)          1600        d5[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1203        batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1203        batchnorm_5[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 642,806\n",
      "Trainable params: 641,878\n",
      "Non-trainable params: 928\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cea132e9d48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m## Call function to train all the models from the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-54196ffca124>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(model_to_test, save_path, SelectFreqs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n\u001b[1;32m     65\u001b[0m                                         patience=400, verbose=0, mode='auto'), \n\u001b[0;32m---> 66\u001b[0;31m                                         CSVLogger(out_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ver_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_Model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Name a folder for the outputs to go into\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "val_results = {'loss':[], 'age_group_loss':[], 'species_loss':[], 'age_group_acc':[], 'species_acc':[]}\n",
    "histories = []\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "# for valid_inc_perc in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "for valid_inc_perc in [0]:\n",
    "    \n",
    "    savedir = (outdir+\"TEST/\")\n",
    "    build_folder(savedir, True)\n",
    "\n",
    "    data_extract = data_loader(valid_inc_perc)\n",
    "    \n",
    "    ## Input CNN Size\n",
    "    input_layer_dim = len(data_extract.X[0])\n",
    "\n",
    "    ## Transform Data\n",
    "    y_age_groups_list = [[age] for age in data_extract.y_age_groups]\n",
    "    y_species_list = [[species] for species in data_extract.y_species]\n",
    "    age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "    age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "    species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "    species_classes = list(np.unique(y_species_list))\n",
    "    y_age_groups_list_vf = [[age] for age in data_extract.y_age_groups_vf]\n",
    "    y_species_list_vf = [[species] for species in data_extract.y_species_vf]\n",
    "    age_groups_vf = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_vf))\n",
    "    species_vf = MultiLabelBinarizer().fit_transform(np.array(y_species_list_vf))\n",
    "\n",
    "    ## Labels\n",
    "    labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "    labels_default_vf, classes_default_vf, outputs_default_vf = [age_groups_vf, species_vf], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "    ## Declare and train the model\n",
    "    model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':2},\n",
    "                 {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':2},\n",
    "    #              {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "                 {'type':'d', 'width':400}]\n",
    "\n",
    "    ## Name the model\n",
    "    model_name = ('Valid_Inc_'+str(valid_inc_perc))\n",
    "\n",
    "    ## Scale train, test\n",
    "    scl = StandardScaler()\n",
    "    features_scl = scl.fit(X=np.vstack((data_extract.X, data_extract.X_vf)))\n",
    "    X_train = features_scl.transform(X=data_extract.X)\n",
    "    X_test = features_scl.transform(X=data_extract.X_vf)\n",
    "\n",
    "    ## Split data into test and train\n",
    "    y_train, y_test = list(map(lambda y:y, labels_default)), list(map(lambda y:y, labels_default_vf))\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "    model_to_test = {\n",
    "        \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "        \"model_name\"  : [model_name],\n",
    "        \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "        \"model_ver_num\"  : [0],\n",
    "        \"fold\"  : [0], # kf.split number on\n",
    "        \"labels\"   : [y_train],\n",
    "        \"features\" : [X_train],\n",
    "        \"classes\"  : [classes_default],\n",
    "        \"outputs\"   : [outputs_default],\n",
    "        \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "        \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "    }\n",
    "\n",
    "    ## Call function to train all the models from the dictionary\n",
    "    model, history = train_models(model_to_test, savedir)\n",
    "    histories.append(history)\n",
    "\n",
    "    predicted_labels = list([] for i in range(len(y_train)))\n",
    "    true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "    y_predicted = model.predict(X_test)\n",
    "    temp_eval = model.evaluate(X_test, y_test)\n",
    "    for metric, res in zip(model.metrics_names, temp_eval):\n",
    "        val_results[metric].append(res)\n",
    "    print(val_results)\n",
    "\n",
    "    predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "    true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "    predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "    true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "    ## Visualize the results\n",
    "    visualize(histories, savedir, model_name, \"0\", classes_default, outputs_default, predicted_labels, true_labels)\n",
    "\n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n",
    "# save_obj(val_results, 'Validation_Results_Dict_4_layers', savedir_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_layer_dim = len(X[0])\n",
    "\n",
    "# # y_age_groups = np.where((y_age<=4), 0, 0)\n",
    "# # y_age_groups = np.where((y_age>=5) & (y_age<=10), 1, y_age_groups)\n",
    "# # y_age_groups = np.where((y_age>=11), 2, y_age_groups)\n",
    "\n",
    "# ## Ages transformed\n",
    "# # y_age_list = [[age] for age in y_age]\n",
    "# y_age_groups_list = [[age] for age in y_age_groups]\n",
    "# y_species_list = [[species] for species in y_species]\n",
    "# y_status_list = [[status] for status in y_status]\n",
    "# # age = MultiLabelBinarizer().fit_transform(np.array(y_age_list))\n",
    "# # age_classes = list(np.unique(y_age_list))\n",
    "# age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "# age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "# species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "# species_classes = list(np.unique(y_species_list))\n",
    "# status = MultiLabelBinarizer().fit_transform(np.array(y_status_list))\n",
    "# status_classes = list(np.unique(y_status_list))\n",
    "\n",
    "# outdir = \"Results_Paper/\"\n",
    "# build_folder(outdir, False)\n",
    "# SelectFreqs = False\n",
    "\n",
    "# ## Labels default - all classification\n",
    "# # labels_default, classes_default, outputs_default = [age, age_groups, species], [age_classes, age_group_classes, species_classes], ['xAge', 'xAgeGroup', 'xSpecies']\n",
    "# labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "# ## Declare and train the model\n",
    "# model_size = [{'type':'c', 'filter':12, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "#              {'type':'c', 'filter':12, 'kernel':8, 'stride':2, 'pooling':2},\n",
    "#              {'type':'c', 'filter':12, 'kernel':3, 'stride':2, 'pooling':2},\n",
    "# #              {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':2},\n",
    "# #              {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "#              {'type':'d', 'width':250}]\n",
    "\n",
    "# ## Name the model\n",
    "# model_name = 'Baseline_CNN'\n",
    "# label = labels_default\n",
    "    \n",
    "# ## Split data into 10 folds for training/testing\n",
    "# kf = KFold(n_splits=10, shuffle=True, random_state=rand_seed)\n",
    "            \n",
    "# ## Features\n",
    "# # features = X\n",
    "    \n",
    "# histories = []\n",
    "# fold = 1\n",
    "# train_model = True\n",
    "\n",
    "# ## Name a folder for the outputs to go into\n",
    "# savedir = (outdir+\"Trian_3_layers/\")\n",
    "# build_folder(savedir, True)\n",
    "    \n",
    "# start_time = time()\n",
    "# save_predicted = []\n",
    "# save_true = []\n",
    "\n",
    "# ## Scale train, test\n",
    "# scl = StandardScaler()\n",
    "# features_scl = scl.fit(X=X)\n",
    "# features = features_scl.transform(X=X)\n",
    "\n",
    "# ## Split into training / testing\n",
    "# test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "# ## Pack up data\n",
    "# X_train = test_splits.pop(0)\n",
    "# X_test = test_splits.pop(0)\n",
    "# y_train = test_splits[::2]\n",
    "# y_test = test_splits[1::2]\n",
    "\n",
    "# if not SelectFreqs:\n",
    "#     X_train = np.expand_dims(X_train, axis=2)\n",
    "#     X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "# model_to_test = {\n",
    "#     \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "#     \"model_name\"  : [model_name],\n",
    "#     \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "#     \"model_ver_num\"  : [0],\n",
    "#     \"fold\"  : [fold], # kf.split number on\n",
    "#     \"labels\"   : [y_train],\n",
    "#     \"features\" : [X_train],\n",
    "#     \"classes\"  : [classes_default],\n",
    "#     \"outputs\"   : [outputs_default],\n",
    "#     \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "#     \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "# }\n",
    "\n",
    "# ## Call function to train all the models from the dictionary\n",
    "# model, history = train_models(model_to_test, savedir, SelectFreqs=SelectFreqs)\n",
    "# histories.append(history)\n",
    "\n",
    "# predicted_labels = list([] for i in range(len(y_train)))\n",
    "# true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "# y_predicted = model.predict(X_test)\n",
    "\n",
    "# predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "# true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "# predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "# true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "# for pred, tru in zip(predicted_labels, true_labels):\n",
    "#     save_predicted.append(pred)\n",
    "#     save_true.append(tru)\n",
    "\n",
    "# ## Visualize the results\n",
    "# visualize(histories, savedir, model_name, str(fold), classes_default, outputs_default, predicted_labels, true_labels)\n",
    "# # log_data(test_index, 'test_index', fold, savedir)\n",
    "\n",
    "# # Clear the Keras session, otherwise it will keep adding new\n",
    "# # models to the same TensorFlow graph each time we create\n",
    "# # a model with a different set of hyper-parameters.\n",
    "# K.clear_session()\n",
    "\n",
    "# # Delete the Keras model with these hyper-parameters from memory.\n",
    "# del model\n",
    "\n",
    "# # visualize(1, savedir, model_name, \"Averaged\", classes_default, outputs_default, save_predicted, save_true)\n",
    "# end_time = time()\n",
    "# print('Run time : {} s'.format(end_time-start_time))\n",
    "# print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "# print('Run time : {} h'.format((end_time-start_time)/3600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "## Size / Accuracy Age / Accuracy Species\n",
    "# size_results = [[200952, 395256, 641878, 1594506],[33,32.7,34.7,32.7],[38.3,38,37.7,38.7]]\n",
    "size_results = [[201, 395, 642, 1595],[33,32.7,34.7,32.7],[38.3,38,37.7,38.7]]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(size_results[0], size_results[1], s=80, c='b', label='Grouped Age Accuracy')\n",
    "plt.scatter(size_results[0], size_results[2], s=80, c='g', label='Species Accuracy')\n",
    "z = np.polyfit(size_results[0], size_results[1], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(size_results[0],p(size_results[0]),\"b--\")\n",
    "z = np.polyfit(size_results[0], size_results[2], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(size_results[0],p(size_results[0]),\"g--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Trainable Parameters (Thousands)\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim([150,1650])\n",
    "plt.savefig('Results_Paper/Molel_Size_Study/Model_Size_Study_Results.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-doing model size reduction plot using accuracy predicted by the model not the confusion matrices.\n",
    "\n",
    "Using confusion matrices doesn't work as it assumes a balanced number of each class in the test set where in reality this is not the case and each accuracy needs scaling by percentage of total test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "3277/3277 [==============================] - 1s 328us/step\n",
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "3277/3277 [==============================] - 1s 337us/step\n",
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "3277/3277 [==============================] - 1s 401us/step\n",
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "3277/3277 [==============================] - 1s 397us/step\n",
      "{'params': [1594506, 641878, 395256, 200952], 'loss': [4.942620224002516, 4.452852591822196, 3.6912602784613995, 3.7235344156172716], 'age_group_loss': [1.9838111671610024, 1.8465644805488934, 1.6836907164275254, 1.5485919786956495], 'species_loss': [2.8875805031818556, 2.4992994878089823, 1.8783734314937144, 1.9791669964171925], 'age_group_acc': [0.34330180042722, 0.35001525785779675, 0.3429966432712847, 0.35398230088495575], 'species_acc': [0.4876411351846201, 0.4833689350015258, 0.480622520598108, 0.480622520598108]}\n"
     ]
    }
   ],
   "source": [
    "val_results = {'params':[], 'loss':[], 'age_group_loss':[], 'species_loss':[], 'age_group_acc':[], 'species_acc':[]}\n",
    "\n",
    "for model_name in ['Train_5_layers/Valid_Inc_0_Model.h5', 'Train_4_layers/Best_Model.h5',\n",
    "                  'Train_3_layers/Valid_Inc_0_Model.h5', 'Train_2_layers/Valid_Inc_0_Model.h5']:\n",
    "\n",
    "    model = load_model(('/home/josh/Documents/Mosquito_Project/MIMI-Analysis/Models/Neural_Networks/CNN/Paper/Results_Paper/Molel_Size_Study/'+model_name))\n",
    "\n",
    "    valid_inc_perc =0\n",
    "\n",
    "    data_extract = data_loader(valid_inc_perc)\n",
    "\n",
    "    ## Input CNN Size\n",
    "    input_layer_dim = len(data_extract.X[0])\n",
    "\n",
    "    ## Transform Data\n",
    "    y_age_groups_list = [[age] for age in data_extract.y_age_groups]\n",
    "    y_species_list = [[species] for species in data_extract.y_species]\n",
    "    age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "    age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "    species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "    species_classes = list(np.unique(y_species_list))\n",
    "    y_age_groups_list_vf = [[age] for age in data_extract.y_age_groups_vf]\n",
    "    y_species_list_vf = [[species] for species in data_extract.y_species_vf]\n",
    "    age_groups_vf = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_vf))\n",
    "    species_vf = MultiLabelBinarizer().fit_transform(np.array(y_species_list_vf))\n",
    "\n",
    "    ## Labels\n",
    "    labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "    labels_default_vf, classes_default_vf, outputs_default_vf = [age_groups_vf, species_vf], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "\n",
    "    ## Scale train, test\n",
    "    scl = StandardScaler()\n",
    "    features_scl = scl.fit(X=np.vstack((data_extract.X, data_extract.X_vf)))\n",
    "    X_train = features_scl.transform(X=data_extract.X)\n",
    "    X_test = features_scl.transform(X=data_extract.X_vf)\n",
    "\n",
    "    ## Split data into test and train\n",
    "    y_train, y_test = list(map(lambda y:y, labels_default)), list(map(lambda y:y, labels_default_vf))\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "\n",
    "    predicted_labels = list([] for i in range(len(y_train)))\n",
    "    true_labels = list([] for i in range(len(y_train)))\n",
    "    \n",
    "    trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "    val_results['params'].append(trainable_count)\n",
    "    \n",
    "    y_predicted = model.predict(X_test)\n",
    "    temp_eval = model.evaluate(X_test, y_test)\n",
    "    for metric, res in zip(model.metrics_names, temp_eval):\n",
    "        val_results[metric].append(res)\n",
    "\n",
    "print(val_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3277, 3)\n",
      "perc AA in test : 0.4107415318889228\n",
      "perc AC in test : 0.07567897467195606\n",
      "perc AG in test : 0.5135794934391211\n"
     ]
    }
   ],
   "source": [
    "print(y_test[1].shape)\n",
    "num_AA = (np.unique(y_test[1][:,0], return_counts=True))[1][1]\n",
    "num_AC = (np.unique(y_test[1][:,1], return_counts=True))[1][1]\n",
    "num_AG = (np.unique(y_test[1][:,2], return_counts=True))[1][1]\n",
    "num_tot = num_AA + num_AC + num_AG\n",
    "print(f'perc AA in test : {num_AA/num_tot}')\n",
    "print(f'perc AC in test : {num_AC/num_tot}')\n",
    "print(f'perc AG in test : {num_AG/num_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "## Size / Accuracy Age / Accuracy Species\n",
    "# size_results = [[200952, 395256, 641878, 1594506],[33,32.7,34.7,32.7],[38.3,38,37.7,38.7]]\n",
    "size_results = [[201, 395, 642, 1595],[33,32.7,34.7,32.7],[38.3,38,37.7,38.7]]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter([val/1000 for val in val_results['params']], [val*100 for val in val_results['age_group_acc']], s=80, c='b', label='Grouped Age Accuracy')\n",
    "plt.scatter([val/1000 for val in val_results['params']], [val*100 for val in val_results['species_acc']], s=80, c='g', label='Species Accuracy')\n",
    "z = np.polyfit([val/1000 for val in val_results['params']], [val*100 for val in val_results['age_group_acc']], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot([val/1000 for val in val_results['params']],p([val/1000 for val in val_results['params']]),\"b--\")\n",
    "z = np.polyfit([val/1000 for val in val_results['params']], [val*100 for val in val_results['species_acc']], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot([val/1000 for val in val_results['params']],p([val/1000 for val in val_results['params']]),\"g--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of Trainable Parameters (Thousands)\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim([150,1650])\n",
    "plt.savefig('Results_Paper/Molel_Size_Study/Model_Size_Study_Results_v2.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
