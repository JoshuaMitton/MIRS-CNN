{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook provides the functionality to build, train, and test a CNN for predicting mosquito age, grouped age, species, and status.\n",
    "\n",
    "## Structure:\n",
    "* Import packages to be used.\n",
    "* Load mosquito data.\n",
    "* Define fucntions for plotting, visualisation, and logging.\n",
    "* Define a function to build the CNN.\n",
    "* Define a function to train the CNN.\n",
    "* Main section to organise data, define the CNN, and call the building and training of the CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import random as rn\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers, metrics\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import model_from_json, load_model\n",
    "from keras.regularizers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# rand_seed = np.random.randint(low=0, high=100)\n",
    "rand_seed = 16\n",
    "print(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "## The below is necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "\n",
    "## The below is necessary for starting core Python generated random numbers in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "## Force TensorFlow to use single thread.\n",
    "## Multiple threads are a potential source of\n",
    "## non-reproducible results.\n",
    "## For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}, intra_op_parallelism_threads=4) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# session_conf = tf.ConfigProto(device_count = {'GPU':0}) #session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "#session_conf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "## The below tf.set_random_seed() will make random number generation\n",
    "## in the TensorFlow backend have a well-defined initial state.\n",
    "## For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "The data file is created using Loco Mosquito:\n",
    "https://github.com/magonji/MIMI-project/blob/master/Loco%20mosquito%204.0.ipynb\n",
    "\n",
    "### The data file has headings: Species - Status - RearCnd - Age - Country- Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "\n",
    "# df['AgeGroup'] = 0\n",
    "# df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "# df['Temp'] = 0\n",
    "# df['Humidity'] = 0\n",
    "\n",
    "# cols = df.columns.tolist()\n",
    "# cols = cols[:6] + cols[-3:] + cols[6:-3]\n",
    "\n",
    "# df = df[cols]\n",
    "\n",
    "# for index, rows in tqdm(df.iterrows()):\n",
    "#     if rows['RearCnd']=='TL':    \n",
    "#         if rows['Country']=='S':\n",
    "#             if rows['Species']=='AA':\n",
    "#                 rows['Temp']=26\n",
    "#                 rows['Humidity']=78\n",
    "#             if rows['Species']=='AC':\n",
    "#                 rows['Temp']=26\n",
    "#                 rows['Humidity']=80\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=26\n",
    "#                 rows['Humidity']=83\n",
    "#         if rows['Country']=='B':\n",
    "#             if rows['Species']=='AC':\n",
    "#                 rows['Temp']=27.5\n",
    "#                 rows['Humidity']=86.1\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=27.5\n",
    "#                 rows['Humidity']=86.1\n",
    "#         if rows['Country']=='T':\n",
    "#             if rows['Species']=='AA':\n",
    "#                 rows['Temp']=27\n",
    "#                 rows['Humidity']=80\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=27\n",
    "#                 rows['Humidity']=80\n",
    "#     if rows['RearCnd']=='TF':    \n",
    "#         if rows['Country']=='B':\n",
    "#             if rows['Species']=='AC':\n",
    "#                 rows['Temp']=30.3\n",
    "#                 rows['Humidity']=64.3\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=30.3\n",
    "#                 rows['Humidity']=64.3\n",
    "#         if rows['Country']=='T':\n",
    "#             if rows['Species']=='AA':\n",
    "#                 rows['Temp']=27\n",
    "#                 rows['Humidity']=80\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=27\n",
    "#                 rows['Humidity']=80\n",
    "#     if rows['RearCnd']=='TV':    \n",
    "#         if rows['Country']=='B':\n",
    "#             if rows['Species']=='AC':\n",
    "#                 rows['Temp']=28.9\n",
    "#                 rows['Humidity']=76.3\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=29\n",
    "#                 rows['Humidity']=73.7\n",
    "#         if rows['Country']=='T':\n",
    "#             if rows['Species']=='AA':\n",
    "#                 rows['Temp']=28.3\n",
    "#                 rows['Humidity']=84.5\n",
    "#             if rows['Species']=='AG':\n",
    "#                 rows['Temp']=28.3\n",
    "#                 rows['Humidity']=84.5\n",
    "#     df.loc[index] = rows\n",
    "                \n",
    "                \n",
    "# print(df.head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0_inc_temp.dat\", '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "# df.head()\n",
    "# print(df.groupby('RearCnd').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_class():\n",
    "    def __init__(self, valid_perc):\n",
    "        df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "        df.head(10)\n",
    "\n",
    "        df['AgeGroup'] = 0\n",
    "        df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "        df_vf = df[df['RearCnd']=='VF']\n",
    "        df_vf = df_vf[df_vf['Status']=='UN']\n",
    "        df = df[df['RearCnd']!='VF']\n",
    "        df = df[df['Status']!='UN']\n",
    "        df_l = df[df['RearCnd']=='TL']\n",
    "        df_l_g = df_l[df_l['Country']=='S']\n",
    "        \n",
    "        df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "        age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "        df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "        age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "        df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "        age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "        df_l_t = df_l[df_l['Country']=='T']\n",
    "        df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "        age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "        df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "        age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "        df_l_b = df_l[df_l['Country']=='B']\n",
    "        df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "        age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "        df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "        age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "        df_f = df[df['RearCnd']=='TF']\n",
    "        df_f_t = df_f[df_f['Country']=='T']\n",
    "        df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "        # df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "        df_f_b = df_f[df_f['Country']=='B']\n",
    "        df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "        age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "        df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "        age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "        df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "        df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "        age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "        age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "        df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "        age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "        df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "        age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "\n",
    "        VF_size_t = len(df_vf_t)\n",
    "        VF_size_b = len(df_vf_b)\n",
    "#         print('validation size tanzania : {}'.format(VF_size_t))\n",
    "#         print('validation size bobo : {}'.format(VF_size_b))\n",
    "        val_group_size_t = int(((((valid_perc*VF_size_t)/2)/3))) #/2 (t/b) /3 (age groups)\n",
    "        val_group_size_b = int(((((valid_perc*VF_size_b)/2)/3)))\n",
    "#         print('validation size for testing tanzania : {}'.format(val_group_size_t))\n",
    "#         print('validation size for testing bobo : {}'.format(val_group_size_b))\n",
    "\n",
    "        size_inc = 400\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            if age == 0:\n",
    "                df_train = df_temp.iloc[index_df_temp_inc]\n",
    "        #         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 400\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 300\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = 300\n",
    "        for age in range(3):\n",
    "            df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "        #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = val_group_size_t\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Tanzania Arabiensis VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            if age == 0:\n",
    "                df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "            else:\n",
    "                df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Tanzania Gambie VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        size_inc = val_group_size_b\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Bobo Gambie VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "        for age in range(3):\n",
    "            df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "            size_df_temp = np.arange(len(df_temp))\n",
    "            if len(size_df_temp) < size_inc:\n",
    "                print('Warning Bobo Colluzzi VF group {} smaller than amount requested'.format(age))\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(size_df_temp)\n",
    "            index_df_temp_inc = size_df_temp[:size_inc]\n",
    "            index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "            df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "            df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "        print('Percentage of field mosquitoes inc {} - Num mosquitoes {} / {}'.format(valid_perc*100, len(df_train[df_train['RearCnd']=='VF']), len(df_vf)))\n",
    "        print('Total number of mosquitoes in the Train set {}'.format(len(df_train)))\n",
    "        \n",
    "        X = df_train.iloc[:,6:-1]\n",
    "        y_age = df_train[\"Age\"]\n",
    "        y_age_groups = df_train[\"AgeGroup\"]\n",
    "        y_species = df_train[\"Species\"]\n",
    "        y_status = df_train[\"Status\"]\n",
    "#         print('shape of X : {}'.format(X.shape))\n",
    "#         print('shape of y age : {}'.format(y_age.shape))\n",
    "#         print('shape of y age groups : {}'.format(y_age_groups.shape))\n",
    "#         print('shape of y species : {}'.format(y_species.shape))\n",
    "#         print('shape of y status : {}'.format(y_status.shape))\n",
    "        self.X = np.asarray(X)\n",
    "        y_age = np.asarray(y_age)\n",
    "        self.y_age_groups = np.asarray(y_age_groups)\n",
    "        self.y_species = np.asarray(y_species)\n",
    "        y_status = np.asarray(y_status)\n",
    "\n",
    "        X_vf = df_test.iloc[:,6:-1]\n",
    "        y_age_vf = df_test[\"Age\"]\n",
    "        y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "        y_species_vf = df_test[\"Species\"]\n",
    "        y_status_vf = df_test[\"Status\"]\n",
    "#         print('shape of X_vf : {}'.format(X_vf.shape))\n",
    "#         print('shape of y_age_vf age : {}'.format(y_age_vf.shape))\n",
    "#         print('shape of y_age_groups_vf : {}'.format(y_age_groups_vf.shape))\n",
    "#         print('shape of y y_species_vf : {}'.format(y_species_vf.shape))\n",
    "#         print('shape of y y_status_vf : {}'.format(y_status_vf.shape))\n",
    "        self.X_vf = np.asarray(X_vf)\n",
    "        y_age_vf = np.asarray(y_age_vf)\n",
    "        self.y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "        self.y_species_vf = np.asarray(y_species_vf)\n",
    "        y_status_vf = np.asarray(y_status_vf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(valid_perc):\n",
    "    return data_loader_class(valid_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "------------------------------------------------------------\n",
      "Percentage of field mosquitoes inc 5.0 - Num mosquitoes 162 / 3277\n",
      "Total number of mosquitoes in the Train set 7362\n",
      "------------------------------------------------------------\n",
      "Percentage of field mosquitoes inc 10.0 - Num mosquitoes 324 / 3277\n",
      "Total number of mosquitoes in the Train set 7524\n",
      "------------------------------------------------------------\n",
      "Percentage of field mosquitoes inc 15.0 - Num mosquitoes 486 / 3277\n",
      "Total number of mosquitoes in the Train set 7686\n",
      "------------------------------------------------------------\n",
      "Percentage of field mosquitoes inc 20.0 - Num mosquitoes 654 / 3277\n",
      "Total number of mosquitoes in the Train set 7854\n",
      "------------------------------------------------------------\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 25.0 - Num mosquitoes 815 / 3277\n",
      "Total number of mosquitoes in the Train set 8015\n",
      "------------------------------------------------------------\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 30.0 - Num mosquitoes 973 / 3277\n",
      "Total number of mosquitoes in the Train set 8173\n",
      "------------------------------------------------------------\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 35.0 - Num mosquitoes 1131 / 3277\n",
      "Total number of mosquitoes in the Train set 8331\n",
      "------------------------------------------------------------\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 40.0 - Num mosquitoes 1294 / 3277\n",
      "Total number of mosquitoes in the Train set 8494\n",
      "------------------------------------------------------------\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 45.0 - Num mosquitoes 1452 / 3277\n",
      "Total number of mosquitoes in the Train set 8652\n",
      "------------------------------------------------------------\n",
      "Warning Tanzania Arabiensis VF group 0 smaller than amount requested\n",
      "Warning Tanzania Gambie VF group 0 smaller than amount requested\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 50.0 - Num mosquitoes 1566 / 3277\n",
      "Total number of mosquitoes in the Train set 8766\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for valid_inc_perc in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    data_loader(valid_inc_perc)\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to create a new folder for the CNN outputs.\n",
    "Useful to stop forgetting to name a new folder when trying out a new model varient and overwriting a days training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folder(fold, to_build = False):\n",
    "    if not os.path.isdir(fold):\n",
    "        if to_build == True:\n",
    "            os.mkdir(fold)\n",
    "        else:\n",
    "            print('Directory does not exists, not creating directory!')\n",
    "    else:\n",
    "        if to_build == True:\n",
    "            raise NameError('Directory already exists, cannot be created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting confusion matrcies\n",
    "This normalizes the confusion matrix and ensures neat plotting for all outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, output, save_path, model_name, fold,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          printout=False):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printout:\n",
    "            print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        if printout:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "    if printout:\n",
    "        print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmin=0, vmax=1) # np.max(np.sum(cm, axis=1)))\n",
    "#     plt.title([title+' - '+model_name])\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "#     plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig((save_path+\"Confusion_Matrix_\"+model_name+\"_\"+fold+\"_\"+output[1:]+\".png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for visualizing outputs\n",
    "This splits the output data into the four categories before plotting the confusion matricies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for visualizing losses and metrics once the neural network fold is trained\n",
    "def visualize(histories, save_path, model_name, fold, classes, outputs, predicted, true, title='Confusion Matrix'):\n",
    "    # Sort out predictions and true labels\n",
    "    for label_predictions_arr, label_true_arr, classes, outputs in zip(predicted, true, classes, outputs):\n",
    "        classes_pred = np.argmax(label_predictions_arr, axis=-1)\n",
    "        classes_true = np.argmax(label_true_arr, axis=-1)\n",
    "        cnf_matrix = confusion_matrix(classes_true, classes_pred)\n",
    "        plot_confusion_matrix(cnf_matrix, classes, outputs, save_path, model_name, fold, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for logging data associated with the model\n",
    "def log_data(log, name, fold, save_path):\n",
    "    f = open((save_path+name+'_'+str(fold)+'_log.txt'), 'w')\n",
    "    np.savetxt(f, log)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name, savedir_main):\n",
    "    with open(savedir_main + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name, savedir_main):\n",
    "    with open(savedir_main + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion for graphing the training data\n",
    "This fucntion creates tidy graphs of loss and accuracy as the models are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_history(history, model_name, model_ver_num, fold, save_path):\n",
    "\n",
    "    font = {'weight' : 'normal',\n",
    "            'size'   : 18}\n",
    "\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    #not_validation = list(filter(lambda x: x[0:3] != \"val\", history.history.keys()))\n",
    "#     print('history.history.keys : {}'.format(history.history.keys()))\n",
    "    filtered = filter(lambda x: x[0:3] != \"val\", history.history.keys())\n",
    "    not_validation = list(filtered)\n",
    "    for i in not_validation:\n",
    "        plt.figure(figsize=(15,7))\n",
    "#         plt.title(i+\"/ \"+\"val_\"+i)\n",
    "        plt.plot(history.history[i], label=i)\n",
    "        plt.plot(history.history[\"val_\"+i], label=\"val_\"+i)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(i)\n",
    "        plt.savefig(save_path +model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+i+\".png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layer):\n",
    "\n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "\n",
    "    for i, layerwidth in zip(range(len(model_shape)),model_shape):\n",
    "        if i == 0:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd) \n",
    "                \n",
    "        else:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                if model_shape[i-1]['type'] == 'c':\n",
    "                    xd = Flatten()(xd)\n",
    "                    \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd)\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "        \n",
    "    \n",
    "#     xAge     = Dense(name = 'age', units = 17, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "    xSpecies = Dense(name ='species', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup', 'xSpecies']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path, SelectFreqs=False):\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "#     input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    label = model_to_test[\"labels\"][0]\n",
    "    features = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "    ## Split into training / testing\n",
    "    test_splits = train_test_split(features, *(label), test_size=0.1, shuffle=True, random_state=rand_seed)\n",
    "    ## Pack up data\n",
    "    X_train = test_splits.pop(0)\n",
    "    X_val = test_splits.pop(0)\n",
    "    y_train = test_splits[::2]\n",
    "    y_val = test_splits[1::2]\n",
    "    \n",
    "    out_model = create_models(model_shape, input_layer_dim)\n",
    "    out_model.summary()\n",
    "    out_history = out_model.fit(x = X_train, \n",
    "                            y = y_train,\n",
    "                            batch_size = 128*16, \n",
    "                            verbose = 0, \n",
    "                            epochs = 8000,\n",
    "                            validation_data = (X_val, y_val),\n",
    "                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                        patience=400, verbose=0, mode='auto'), \n",
    "                                        CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "    scores = out_model.evaluate(X_val, y_val)\n",
    "#     print(out_model.metrics_names)\n",
    "    \n",
    "#     ## Kfold training\n",
    "#     seed = rand_seed\n",
    "#     kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#     ## Split data into test and train\n",
    "    \n",
    "#     model_ver_num = 0\n",
    "#     cv_scores = []\n",
    "#     best_score = 0\n",
    "#     for train_index, val_index in kfold.split(features):\n",
    "#         print('Fold {} Running'.format(model_ver_num))\n",
    "        \n",
    "#         X_train, X_val = features[train_index], features[val_index]\n",
    "#         y_train, y_val = list(map(lambda y:y[train_index], label)), list(map(lambda y:y[val_index], label))\n",
    "\n",
    "#         model = create_models(model_shape, input_layer_dim)\n",
    "#         if model_ver_num == 0:\n",
    "#             model.summary()\n",
    "\n",
    "#         history = model.fit(x = X_train, \n",
    "#                             y = y_train,\n",
    "#                             batch_size = 128*16, \n",
    "#                             verbose = 0, \n",
    "#                             epochs = 8000,\n",
    "#                             validation_data = (X_val, y_val),\n",
    "#                             callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                         patience=400, verbose=0, mode='auto'), \n",
    "#                                         CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "#         scores = model.evaluate(X_val, y_val)\n",
    "#         print(model.metrics_names)\n",
    "#         print(scores)\n",
    "#         if (scores[3] + scores[4]) > best_score:\n",
    "#             out_model = model\n",
    "#             out_history = history\n",
    "        \n",
    "#         model_ver_num += 1\n",
    "        \n",
    "#         # Clear the Keras session, otherwise it will keep adding new\n",
    "#         # models to the same TensorFlow graph each time we create\n",
    "#         # a model with a different set of hyper-parameters.\n",
    "#         K.clear_session()\n",
    "\n",
    "#         # Delete the Keras model with these hyper-parameters from memory.\n",
    "#         del model\n",
    "        \n",
    "    out_model.save((save_path+model_name+\"_\"+'Model.h5'))\n",
    "    graph_history(out_history, model_name, 0, 0, save_path)\n",
    "    \n",
    "    return out_model, out_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main section\n",
    "\n",
    "Functionality:\n",
    "* Oganises the data into a format of lists of data, classes, labels.\n",
    "* Define the CNN to be built.\n",
    "* Define the KFold validation to be used.\n",
    "* Build a folder to output data into.\n",
    "* Standardize and oragnise data into training/testing.\n",
    "* Call the model training.\n",
    "* Organize outputs and call visualization for plotting and graphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of field mosquitoes inc 0 - Num mosquitoes 0 / 3277\n",
      "Total number of mosquitoes in the Train set 7200\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "720/720 [==============================] - 0s 262us/step\n",
      "3277/3277 [==============================] - 1s 163us/step\n",
      "{'loss': [4.942620219346187], 'age_group_loss': [1.9838111671610024], 'species_loss': [2.8875805031818556], 'age_group_acc': [0.34330180042722], 'species_acc': [0.4876411351846201]}\n",
      "Percentage of field mosquitoes inc 5.0 - Num mosquitoes 162 / 3277\n",
      "Total number of mosquitoes in the Train set 7362\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737/737 [==============================] - 0s 210us/step\n",
      "3115/3115 [==============================] - 0s 152us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363], 'age_group_loss': [1.9838111671610024, 1.2416391557832973], 'species_loss': [2.8875805031818556, 1.172873507935201], 'age_group_acc': [0.34330180042722, 0.5338683788121991], 'species_acc': [0.4876411351846201, 0.5852327448120087]}\n",
      "Percentage of field mosquitoes inc 10.0 - Num mosquitoes 324 / 3277\n",
      "Total number of mosquitoes in the Train set 7524\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "753/753 [==============================] - 0s 141us/step\n",
      "2953/2953 [==============================] - 0s 69us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038]}\n",
      "Percentage of field mosquitoes inc 15.0 - Num mosquitoes 486 / 3277\n",
      "Total number of mosquitoes in the Train set 7686\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/769 [==============================] - 0s 134us/step\n",
      "2791/2791 [==============================] - 0s 90us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165]}\n",
      "Percentage of field mosquitoes inc 20.0 - Num mosquitoes 654 / 3277\n",
      "Total number of mosquitoes in the Train set 7854\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "786/786 [==============================] - 0s 140us/step\n",
      "2623/2623 [==============================] - 0s 98us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417]}\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 25.0 - Num mosquitoes 815 / 3277\n",
      "Total number of mosquitoes in the Train set 8015\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802/802 [==============================] - 0s 147us/step\n",
      "2462/2462 [==============================] - 0s 100us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213]}\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 30.0 - Num mosquitoes 973 / 3277\n",
      "Total number of mosquitoes in the Train set 8173\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "818/818 [==============================] - 0s 126us/step\n",
      "2304/2304 [==============================] - 0s 106us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501, 0.9535149013002714], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391, 0.4801018638536334], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023, 0.4021792692753176], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213, 0.8285590277777778]}\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 35.0 - Num mosquitoes 1131 / 3277\n",
      "Total number of mosquitoes in the Train set 8331\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834/834 [==============================] - 0s 152us/step\n",
      "2146/2146 [==============================] - 0s 109us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501, 0.9535149013002714, 0.8382450232190277], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391, 0.4801018638536334, 0.42035708500754576], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023, 0.4021792692753176, 0.3469646150334795], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213, 0.8285590277777778, 0.863932898415657]}\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 40.0 - Num mosquitoes 1294 / 3277\n",
      "Total number of mosquitoes in the Train set 8494\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "850/850 [==============================] - 0s 133us/step\n",
      "1983/1983 [==============================] - 0s 86us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501, 0.9535149013002714, 0.8382450232190277, 0.7638672286070661], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391, 0.4801018638536334, 0.42035708500754576, 0.3933762730340193], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023, 0.4021792692753176, 0.3469646150334795, 0.2996454734544951], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213, 0.8285590277777778, 0.863932898415657, 0.8835098337958812]}\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 45.0 - Num mosquitoes 1452 / 3277\n",
      "Total number of mosquitoes in the Train set 8652\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866/866 [==============================] - 0s 150us/step\n",
      "1825/1825 [==============================] - 0s 97us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501, 0.9535149013002714, 0.8382450232190277, 0.7638672286070661, 0.6683299169148484], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391, 0.4801018638536334, 0.42035708500754576, 0.3933762730340193, 0.3253837525273023], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023, 0.4021792692753176, 0.3469646150334795, 0.2996454734544951, 0.2716564917890993], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262, 0.8717808219178083], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213, 0.8285590277777778, 0.863932898415657, 0.8835098337958812, 0.8936986301369862]}\n",
      "Warning Tanzania Arabiensis VF group 0 smaller than amount requested\n",
      "Warning Tanzania Gambie VF group 0 smaller than amount requested\n",
      "Warning Bobo Colluzzi VF group 2 smaller than amount requested\n",
      "Percentage of field mosquitoes inc 50.0 - Num mosquitoes 1566 / 3277\n",
      "Total number of mosquitoes in the Train set 8766\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1625, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv1D)                  (None, 1618, 16)     144         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_1 (BatchNormalization (None, 1618, 16)     64          Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1618, 16)     0           batchnorm_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv1D)                  (None, 806, 16)      2064        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_2 (BatchNormalization (None, 806, 16)      64          Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 806, 16)      0           batchnorm_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv3 (Conv1D)                  (None, 804, 16)      784         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_3 (BatchNormalization (None, 804, 16)      64          Conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 804, 16)      0           batchnorm_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv4 (Conv1D)                  (None, 400, 16)      1552        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_4 (BatchNormalization (None, 400, 16)      64          Conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 400, 16)      0           batchnorm_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Conv5 (Conv1D)                  (None, 396, 16)      1296        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_5 (BatchNormalization (None, 396, 16)      64          Conv5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 198, 16)      0           batchnorm_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dout6 (Dropout)                 (None, 3168)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "d6 (Dense)                      (None, 500)          1584500     dout6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm_6 (BatchNormalization (None, 500)          2000        d6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 3)            1503        batchnorm_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "species (Dense)                 (None, 3)            1503        batchnorm_6[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,595,666\n",
      "Trainable params: 1,594,506\n",
      "Non-trainable params: 1,160\n",
      "__________________________________________________________________________________________________\n",
      "877/877 [==============================] - 0s 122us/step\n",
      "1711/1711 [==============================] - 0s 104us/step\n",
      "{'loss': [4.942620219346187, 2.4815747823225363, 1.874446988065405, 1.484810124349953, 1.2420932128599387, 1.0482862967964501, 0.9535149013002714, 0.8382450232190277, 0.7638672286070661, 0.6683299169148484, 0.6074700060119272], 'age_group_loss': [1.9838111671610024, 1.2416391557832973, 0.9549200318758766, 0.7446512885569234, 0.6229374602783558, 0.5180432014984391, 0.4801018638536334, 0.42035708500754576, 0.3933762730340193, 0.3253837525273023, 0.2941282957983881], 'species_loss': [2.8875805031818556, 1.172873507935201, 0.8500014498976178, 0.6701478279969548, 0.5495299646702605, 0.4602895513189023, 0.4021792692753176, 0.3469646150334795, 0.2996454734544951, 0.2716564917890993, 0.24230051141674236], 'age_group_acc': [0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262, 0.8717808219178083, 0.8965517242424395], 'species_acc': [0.4876411351846201, 0.5852327448120087, 0.662038604849038, 0.7216051594624165, 0.7651544032640417, 0.8050365556579213, 0.8285590277777778, 0.863932898415657, 0.8835098337958812, 0.8936986301369862, 0.9117475162814891]}\n",
      "Run time : 52187.2268280983 s\n",
      "Run time : 869.7871138016383 m\n",
      "Run time : 14.496451896693971 h\n"
     ]
    }
   ],
   "source": [
    "## Name a folder for the outputs to go into\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir_main = (outdir+\"Trian_Val_Inc/\")\n",
    "build_folder(savedir_main, True)\n",
    "\n",
    "val_results = {'loss':[], 'age_group_loss':[], 'species_loss':[], 'age_group_acc':[], 'species_acc':[]}\n",
    "histories = []\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "for valid_inc_perc in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    \n",
    "    savedir = (savedir_main+\"valid_data_inc_\"+str(valid_inc_perc)+\"/\")\n",
    "    build_folder(savedir, True)\n",
    "\n",
    "    data_extract = data_loader(valid_inc_perc)\n",
    "    \n",
    "    ## Input CNN Size\n",
    "    input_layer_dim = len(data_extract.X[0])\n",
    "\n",
    "    ## Transform Data\n",
    "    y_age_groups_list = [[age] for age in data_extract.y_age_groups]\n",
    "    y_species_list = [[species] for species in data_extract.y_species]\n",
    "    age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "    age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "    species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "    species_classes = list(np.unique(y_species_list))\n",
    "    y_age_groups_list_vf = [[age] for age in data_extract.y_age_groups_vf]\n",
    "    y_species_list_vf = [[species] for species in data_extract.y_species_vf]\n",
    "    age_groups_vf = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_vf))\n",
    "    species_vf = MultiLabelBinarizer().fit_transform(np.array(y_species_list_vf))\n",
    "\n",
    "    ## Labels\n",
    "    labels_default, classes_default, outputs_default = [age_groups, species], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "    labels_default_vf, classes_default_vf, outputs_default_vf = [age_groups_vf, species_vf], [age_group_classes, species_classes], ['xAgeGroup', 'xSpecies']\n",
    "\n",
    "    ## Declare and train the model\n",
    "    model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "                 {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "                 {'type':'d', 'width':500}]\n",
    "\n",
    "    ## Name the model\n",
    "    model_name = ('Valid_Inc_'+str(valid_inc_perc))\n",
    "\n",
    "    ## Scale train, test\n",
    "    scl = StandardScaler()\n",
    "    features_scl = scl.fit(X=np.vstack((data_extract.X, data_extract.X_vf)))\n",
    "    X_train = features_scl.transform(X=data_extract.X)\n",
    "    X_test = features_scl.transform(X=data_extract.X_vf)\n",
    "\n",
    "    ## Split data into test and train\n",
    "    y_train, y_test = list(map(lambda y:y, labels_default)), list(map(lambda y:y, labels_default_vf))\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "    model_to_test = {\n",
    "        \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "        \"model_name\"  : [model_name],\n",
    "        \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "        \"model_ver_num\"  : [0],\n",
    "        \"fold\"  : [0], # kf.split number on\n",
    "        \"labels\"   : [y_train],\n",
    "        \"features\" : [X_train],\n",
    "        \"classes\"  : [classes_default],\n",
    "        \"outputs\"   : [outputs_default],\n",
    "        \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "        \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "    }\n",
    "\n",
    "    ## Call function to train all the models from the dictionary\n",
    "    model, history = train_models(model_to_test, savedir)\n",
    "    histories.append(history)\n",
    "\n",
    "    predicted_labels = list([] for i in range(len(y_train)))\n",
    "    true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "    y_predicted = model.predict(X_test)\n",
    "    temp_eval = model.evaluate(X_test, y_test)\n",
    "    for metric, res in zip(model.metrics_names, temp_eval):\n",
    "        val_results[metric].append(res)\n",
    "    print(val_results)\n",
    "\n",
    "    predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "    true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "    predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "    true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "    ## Visualize the results\n",
    "    visualize(histories, savedir, model_name, \"0\", classes_default, outputs_default, predicted_labels, true_labels)\n",
    "\n",
    "    # Clear the Keras session, otherwise it will keep adding new\n",
    "    # models to the same TensorFlow graph each time we create\n",
    "    # a model with a different set of hyper-parameters.\n",
    "    K.clear_session()\n",
    "\n",
    "    # Delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n",
    "save_obj(val_results, 'Validation_Results_Dict', savedir_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262, 0.8717808219178083, 0.8965517242424395]\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir_main = (outdir+\"Trian_Val_Inc/\")\n",
    "build_folder(savedir_main, False)\n",
    "\n",
    "val_results = load_obj('Validation_Results_Dict', savedir_main)\n",
    "print(val_results[\"age_group_acc\"])\n",
    "# x = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "x = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# fig.suptitle('Time Course Validation Data Inclusion Study', fontsize=20, fontweight='bold')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, val_results[\"age_group_acc\"][:-1], lw=2, c='b', label=\"Age Group Acc\")\n",
    "ax.plot(x, val_results[\"species_acc\"][:-1], lw=2, c='g', label=\"Species Acc\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel(\"Percentage of Time Course Validation Data Included During Training\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "ax.set_xticks([0,5,10,15,20,25,30,35,40,45])\n",
    "# ax.set_xlim([0,45])\n",
    "ax.set_ylim([0.2,1])\n",
    "for i,j in zip(x, val_results[\"age_group_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j-0.06), color='b', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "for i,j in zip(x, val_results[\"species_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j+0.06), color='g', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "plt.savefig(savedir_main + \"Validation_Study.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262, 0.8717808219178083, 0.8965517242424395]\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir_main = (outdir+\"Trian_Val_Inc/\")\n",
    "build_folder(savedir_main, False)\n",
    "\n",
    "val_results = load_obj('Validation_Results_Dict', savedir_main)\n",
    "print(val_results[\"age_group_acc\"])\n",
    "# x = [0, 2.2, 4.3, 6.3, 8.3, 10.2, 11.9, 13.6, 15.2, 16.8, 17.9]\n",
    "x = [0, 2.2, 4.3, 6.3, 8.3, 10.2, 11.9, 13.6, 15.2, 16.8]\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# fig.suptitle('Time Course Validation Data Inclusion Study', fontsize=20, fontweight='bold')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, val_results[\"age_group_acc\"][:-1], lw=2, c='b', label=\"Age Group Acc\")\n",
    "ax.plot(x, val_results[\"species_acc\"][:-1], lw=2, c='g', label=\"Species Acc\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel(\"Proportion of Semifeld Data in the Total Training Dataset\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "# ax.set_xticks([0, 2.2, 4.3, 6.3, 8.3, 10.2, 11.9, 13.6, 15.2, 16.8])\n",
    "# ax.set_xlim([0,17.6])\n",
    "ax.set_ylim([0.2,1])\n",
    "for i,j in zip(x, val_results[\"age_group_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j-0.06), color='b', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "for i,j in zip(x, val_results[\"species_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j+0.06), color='g', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "plt.savefig(savedir_main + \"Validation_Study.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34330180042722, 0.5338683788121991, 0.6352861496782932, 0.6864922966678609, 0.7483797177780325, 0.7806661254404813, 0.8038194444444444, 0.8355079217148182, 0.8567826522761262, 0.8717808219178083, 0.8965517242424395]\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "outdir = \"Results_Paper/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir_main = (outdir+\"Trian_Val_Inc/\")\n",
    "build_folder(savedir_main, False)\n",
    "\n",
    "val_results = load_obj('Validation_Results_Dict', savedir_main)\n",
    "print(val_results[\"age_group_acc\"])\n",
    "# x = [0, 162, 324, 486, 654, 815, 973, 1131, 1294, 1452, 1566]\n",
    "x = [0, 162, 324, 486, 654, 815, 973, 1131, 1294, 1452]\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "# fig.suptitle('Time Course Validation Data Inclusion Study', fontsize=20, fontweight='bold')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, val_results[\"age_group_acc\"][:-1], lw=2, c='b', label=\"Age Group Acc\")\n",
    "ax.plot(x, val_results[\"species_acc\"][:-1], lw=2, c='g', label=\"Species Acc\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel(\"Number of Semifeld Datapoints in the Total Training Dataset (+ 7200 Non-Semifield Datapoints)\")\n",
    "ax.set_ylabel(\"Test Accuracy\")\n",
    "# ax.set_xticks([0, 2.2, 4.3, 6.3, 8.3, 10.2, 11.9, 13.6, 15.2, 16.8])\n",
    "# ax.set_xlim([0,17.6])\n",
    "ax.set_ylim([0.2,1])\n",
    "for i,j in zip(x, val_results[\"age_group_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j-0.06), color='b', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "for i,j in zip(x, val_results[\"species_acc\"][:-1]):\n",
    "    ax.annotate('{:1.2f}'.format(j), xy=(i,j), xytext=(i+0,j+0.06), color='g', arrowprops=dict(facecolor='black', width=1, headwidth=4, headlength=4, shrink=0.05))\n",
    "plt.savefig(savedir_main + \"Validation_Study.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave one out TimeCourseValidation\n",
    "### Validation with Gambie only + 2 African sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funciton to create the CNN\n",
    "This function takes as an input a list of dictionaries. Each element in the list is a new hidden layer in the model. For each layer the dictionary defines the layer to be used.\n",
    "\n",
    "### Available options are:\n",
    "Convolutional Layer:\n",
    "* type = 'c'\n",
    "* filter = optional number of filters\n",
    "* kernel = optional size of the filters\n",
    "* stride = optional size of stride to take between filters\n",
    "* pooling = optional width of the max pooling\n",
    "* {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2}\n",
    "\n",
    "dense layer:\n",
    "* type = 'd'\n",
    "* width = option width of the layer\n",
    "* {'type':'d', 'width':500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(model_shape, input_layerTrian_no_TClab_TCfield_20vf_dim):\n",
    "\n",
    "    regConst = 0.02\n",
    "    sgd = keras.optimizers.SGD(lr=0.003, decay=1e-5, momentum=0.9, nesterov=True, clipnorm=1.)\n",
    "    cce = 'categorical_crossentropy'\n",
    "\n",
    "    input_vec = Input(name='input', shape=(input_layer_dim,1))\n",
    "\n",
    "    for i, layerwidth in zip(range(len(model_shape)),model_shape):\n",
    "        if i == 0:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(input_vec)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd) \n",
    "                \n",
    "        else:\n",
    "            if model_shape[i]['type'] == 'c':\n",
    "                xd = Conv1D(name=('Conv'+str(i+1)), filters=model_shape[i]['filter'], \n",
    "                 kernel_size = model_shape[i]['kernel'], strides = model_shape[i]['stride'],\n",
    "                 activation = 'relu',\n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd)\n",
    "                xd = MaxPooling1D(pool_size=(model_shape[i]['pooling']))(xd)\n",
    "                \n",
    "            elif model_shape[i]['type'] == 'd':\n",
    "                if model_shape[i-1]['type'] == 'c':\n",
    "                    xd = Flatten()(xd)\n",
    "                    \n",
    "                xd = Dropout(name=('dout'+str(i+1)), rate=0.5)(xd)\n",
    "                xd = Dense(name=('d'+str(i+1)), units=model_shape[i]['width'], activation='relu', \n",
    "                 kernel_regularizer=l2(regConst), \n",
    "                 kernel_initializer='he_normal')(xd)\n",
    "                xd = BatchNormalization(name=('batchnorm_'+str(i+1)))(xd) \n",
    "        \n",
    "    \n",
    "#     xAge     = Dense(name = 'age', units = 17, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "    xAgeGroup     = Dense(name = 'age_group', units = 3, \n",
    "                     activation = 'softmax', \n",
    "                     kernel_regularizer = l2(regConst), \n",
    "                     kernel_initializer = 'he_normal')(xd)\n",
    "#     xSpecies = Dense(name ='species', units = 1, \n",
    "#                      activation = 'softmax', \n",
    "#                      kernel_regularizer = l2(regConst), \n",
    "#                      kernel_initializer = 'he_normal')(xd)\n",
    "\n",
    "    outputs = []\n",
    "#     for i in ['xAge', 'xAgeGroup', 'xSpecies']:\n",
    "    for i in ['xAgeGroup']:\n",
    "        outputs.append(locals()[i])\n",
    "    model = Model(inputs = input_vec, outputs = outputs)\n",
    "    \n",
    "    model.compile(loss=cce, metrics=['acc'], \n",
    "                  optimizer=sgd)\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model\n",
    "\n",
    "This function will split the data into training and validation and call the create models function. This fucntion returns the model and training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_to_test, save_path):\n",
    "\n",
    "    model_shape = model_to_test[\"model_shape\"][0]\n",
    "    model_name = model_to_test[\"model_name\"][0]\n",
    "    input_layer_dim = model_to_test[\"input_layer_dim\"][0]\n",
    "    model_ver_num = model_to_test[\"model_ver_num\"][0]\n",
    "    fold = model_to_test[\"fold\"][0]\n",
    "    y_train = model_to_test[\"labels\"][0]\n",
    "    X_train = model_to_test[\"features\"][0]\n",
    "    classes = model_to_test[\"classes\"][0]\n",
    "    outputs = model_to_test[\"outputs\"][0]\n",
    "    compile_loss = model_to_test[\"compile_loss\"][0]\n",
    "    compile_metrics = model_to_test[\"compile_metrics\"][0]\n",
    "\n",
    "    ## Split into training / testing\n",
    "    test_splits = train_test_split(X_train, *(y_train), test_size=0.4, shuffle=True, random_state=42)\n",
    "    ## Pack up data\n",
    "    X_train = test_splits.pop(0)\n",
    "    X_val = test_splits.pop(0)\n",
    "    y_train = test_splits[::2]\n",
    "    y_val = test_splits[1::2]\n",
    "\n",
    "    model = create_models(model_shape, input_layer_dim)\n",
    "#     model.summary()\n",
    "    \n",
    "    history = model.fit(x = X_train, \n",
    "                        y = y_train,\n",
    "                        batch_size = 128*16, \n",
    "                        verbose = 0, \n",
    "                        epochs = 8000,\n",
    "                        validation_data = (X_val, y_val),\n",
    "                        callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                    patience=400, verbose=0, mode='auto'), \n",
    "                                    CSVLogger(save_path+model_name+\"_\"+str(model_ver_num)+'.csv', append=True, separator=';')])\n",
    "\n",
    "    model.save((save_path+model_name+\"_\"+str(model_ver_num)+\"_\"+str(fold)+\"_\"+'Model.h5'))\n",
    "    graph_history(history, model_name, model_ver_num, fold, save_path)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave out Bobo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "\n",
    "\n",
    "size_inc = 400\n",
    "# for age in range(3):\n",
    "#     df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     if age == 0:\n",
    "#         df_train = df_temp.iloc[index_df_temp_inc]\n",
    "# #         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "#     else:\n",
    "#         df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "# for age in range(3):\n",
    "#     df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 300\n",
    "# for age in range(3):\n",
    "#     df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 300\n",
    "# for age in range(3):\n",
    "#     df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 70\n",
    "# for age in range(3):\n",
    "#     df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     if len(size_df_temp) < size_inc:\n",
    "#         print('Warning Tanzania Arabiensis VF group {} smaller than amount requested'.format(age))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     if age == 0:\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "#     else:\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    if len(size_df_temp) < size_inc:\n",
    "        print('Warning Tanzania Gambie VF group {} smaller than amount requested'.format(age))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    if len(size_df_temp) < size_inc:\n",
    "        print('Warning Bobo Gambie VF group {} smaller than amount requested'.format(age))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "# for age in range(3):\n",
    "#     df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     if len(size_df_temp) < size_inc:\n",
    "#         print('Warning Bobo Colluzzi VF group {} smaller than amount requested'.format(age))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "# #     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3510, 1625)\n",
      "(265, 1625)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_vf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time : 1511.5541138648987 s\n",
      "Run time : 25.192568564414977 m\n",
      "Run time : 0.41987614274024965 h\n"
     ]
    }
   ],
   "source": [
    "## Name a folder for the outputs to go into\n",
    "outdir = \"output_data_update_19_02/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir = (outdir+\"Paper_Results/Trian_Val_No_Bobo/\")\n",
    "build_folder(savedir, True)\n",
    "\n",
    "val_results = {'loss':[], 'age_group_loss':[], 'species_loss':[], 'age_group_acc':[], 'species_acc':[]}\n",
    "histories = []\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "## Input CNN Size\n",
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Transform Data\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "y_age_groups_list_vf = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_vf = [[species] for species in y_species_vf]\n",
    "age_groups_vf = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_vf))\n",
    "species_vf = MultiLabelBinarizer().fit_transform(np.array(y_species_list_vf))\n",
    "\n",
    "## Labels\n",
    "labels_default, classes_default, outputs_default = [age_groups], [age_group_classes], ['xAgeGroup']\n",
    "labels_default_vf, classes_default_vf, outputs_default_vf = [age_groups_vf], [age_group_classes], ['xAgeGroup']\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = ('Valid_Inc_No_Bobo')\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "X_train = features_scl.transform(X=X)\n",
    "X_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split data into test and train\n",
    "y_train, y_test = list(map(lambda y:y, labels_default)), list(map(lambda y:y, labels_default_vf))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [0], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = [y_predicted]\n",
    "# temp_eval = model.evaluate(X_test, y_test)\n",
    "# for metric, res in zip(model.metrics_names, temp_eval):\n",
    "#     val_results[metric].append(res)\n",
    "# print(val_results)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, \"0\", classes_default, outputs_default, predicted_labels, true_labels, title='Prediction on Bobo Validation Field')\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n",
    "save_obj(history.history, 'Validation_Results_History', savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave out Ifakara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning Bobo Gambie VF group 2 smaller than amount requested\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/josh/Documents/Mosquito_Project/New_Data/Data/MIMIdata_update_19_02/mosquitoes_country_LM_5_0.dat\", '\\t')\n",
    "df.head(10)\n",
    "\n",
    "df['AgeGroup'] = 0\n",
    "df['AgeGroup'] = np.where(df['Age']>10, 2, np.where(df['Age']>4, 1, 0))\n",
    "\n",
    "df_vf = df[df['RearCnd']=='VF']\n",
    "df_vf = df_vf[df_vf['Status']=='UN']\n",
    "df = df[df['RearCnd']!='VF']\n",
    "df = df[df['Status']!='UN']\n",
    "df_l = df[df['RearCnd']=='TL']\n",
    "df_l_g = df_l[df_l['Country']=='S']\n",
    "df_l_g_a = df_l_g[df_l_g['Species']=='AA']\n",
    "age_counts = df_l_g_a.groupby('AgeGroup').size()\n",
    "df_l_g_g = df_l_g[df_l_g['Species']=='AG']\n",
    "age_counts = df_l_g_g.groupby('AgeGroup').size()\n",
    "df_l_g_c = df_l_g[df_l_g['Species']=='AC']\n",
    "age_counts = df_l_g_c.groupby('AgeGroup').size()\n",
    "df_l_t = df_l[df_l['Country']=='T']\n",
    "df_l_t_a = df_l_t[df_l_t['Species']=='AA']\n",
    "age_counts = df_l_t_a.groupby('AgeGroup').size()\n",
    "df_l_t_g = df_l_t[df_l_t['Species']=='AG']\n",
    "age_counts = df_l_t_g.groupby('AgeGroup').size()\n",
    "df_l_b = df_l[df_l['Country']=='B']\n",
    "df_l_b_g = df_l_b[df_l_b['Species']=='AG']\n",
    "age_counts = df_l_b_g.groupby('AgeGroup').size()\n",
    "df_l_b_c = df_l_b[df_l_b['Species']=='AC']\n",
    "age_counts = df_l_b_c.groupby('AgeGroup').size()\n",
    "df_f = df[df['RearCnd']=='TF']\n",
    "df_f_t = df_f[df_f['Country']=='T']\n",
    "df_f_t_a = df_f_t[df_f_t['Species']=='AA']\n",
    "# df_f_t_g = df_f_t[df_f_t['Species']=='AG'] #There isn't any\n",
    "df_f_b = df_f[df_f['Country']=='B']\n",
    "df_f_b_g = df_f_b[df_f_b['Species']=='AG']\n",
    "age_counts = df_f_b_g.groupby('AgeGroup').size()\n",
    "df_f_b_c = df_f_b[df_f_b['Species']=='AC']\n",
    "age_counts = df_f_b_c.groupby('AgeGroup').size()\n",
    "df_vf_t = df_vf[df_vf['Country']=='T']\n",
    "df_vf_t_a = df_vf_t[df_vf_t['Species']=='AA']\n",
    "age_counts = df_vf_t_a.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_t_g = df_vf_t[df_vf_t['Species']=='AG']\n",
    "age_counts = df_vf_t_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_b = df_vf[df_vf['Country']=='B']\n",
    "df_vf_b_g = df_vf_b[df_vf_b['Species']=='AG']\n",
    "age_counts = df_vf_b_g.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "df_vf_b_c = df_vf_b[df_vf_b['Species']=='AC']\n",
    "age_counts = df_vf_b_c.groupby('AgeGroup').size()\n",
    "#         print(age_counts)\n",
    "\n",
    "\n",
    "size_inc = 400\n",
    "# for age in range(3):\n",
    "#     df_temp = df_l_t_a[df_l_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     if age == 0:\n",
    "#         df_train = df_temp.iloc[index_df_temp_inc]\n",
    "# #         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "#     else:\n",
    "#         df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_l_t_g[df_l_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    if age == 0:\n",
    "        df_train = df_temp.iloc[index_df_temp_inc]\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 400\n",
    "for age in range(3):\n",
    "    df_temp = df_l_b_g[df_l_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "# for age in range(3):\n",
    "#     df_temp = df_l_b_c[df_l_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 300\n",
    "# for age in range(3):\n",
    "#     df_temp = df_f_t_a[df_f_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_f_b_g[df_f_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 300\n",
    "# for age in range(3):\n",
    "#     df_temp = df_f_b_c[df_f_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "# #     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 0\n",
    "# for age in range(3):\n",
    "#     df_temp = df_vf_t_a[df_vf_t_a['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     if len(size_df_temp) < size_inc:\n",
    "#         print('Warning Tanzania Arabiensis VF group {} smaller than amount requested'.format(age))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     if age == 0:\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "#     else:\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_t_g[df_vf_t_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    if len(size_df_temp) < size_inc:\n",
    "        print('Warning Tanzania Gambie VF group {} smaller than amount requested'.format(age))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "#     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "    if age == 0:\n",
    "        df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "    else:\n",
    "        df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "size_inc = 70\n",
    "for age in range(3):\n",
    "    df_temp = df_vf_b_g[df_vf_b_g['AgeGroup']==age]\n",
    "    size_df_temp = np.arange(len(df_temp))\n",
    "    if len(size_df_temp) < size_inc:\n",
    "        print('Warning Bobo Gambie VF group {} smaller than amount requested'.format(age))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(size_df_temp)\n",
    "    index_df_temp_inc = size_df_temp[:size_inc]\n",
    "    index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "    df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     if age == 0:\n",
    "#         df_test = df_temp.iloc[index_df_temp_not_inc]\n",
    "#     else:\n",
    "#         df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "# for age in range(3):\n",
    "#     df_temp = df_vf_b_c[df_vf_b_c['AgeGroup']==age]\n",
    "#     size_df_temp = np.arange(len(df_temp))\n",
    "#     if len(size_df_temp) < size_inc:\n",
    "#         print('Warning Bobo Colluzzi VF group {} smaller than amount requested'.format(age))\n",
    "#     np.random.seed(42)\n",
    "#     np.random.shuffle(size_df_temp)\n",
    "#     index_df_temp_inc = size_df_temp[:size_inc]\n",
    "#     index_df_temp_not_inc = size_df_temp[size_inc:]\n",
    "# #     df_train = pd.concat([df_train, df_temp.iloc[index_df_temp_inc]])\n",
    "#     df_test = pd.concat([df_test, df_temp.iloc[index_df_temp_not_inc]])\n",
    "\n",
    "X = df_train.iloc[:,6:-1]\n",
    "y_age = df_train[\"Age\"]\n",
    "y_age_groups = df_train[\"AgeGroup\"]\n",
    "y_species = df_train[\"Species\"]\n",
    "y_status = df_train[\"Status\"]\n",
    "X = np.asarray(X)\n",
    "y_age = np.asarray(y_age)\n",
    "y_age_groups = np.asarray(y_age_groups)\n",
    "y_species = np.asarray(y_species)\n",
    "y_status = np.asarray(y_status)\n",
    "\n",
    "X_vf = df_test.iloc[:,6:-1]\n",
    "y_age_vf = df_test[\"Age\"]\n",
    "y_age_groups_vf = df_test[\"AgeGroup\"]\n",
    "y_species_vf = df_test[\"Species\"]\n",
    "y_status_vf = df_test[\"Status\"]\n",
    "X_vf = np.asarray(X_vf)\n",
    "y_age_vf = np.asarray(y_age_vf)\n",
    "y_age_groups_vf = np.asarray(y_age_groups_vf)\n",
    "y_species_vf = np.asarray(y_species_vf)\n",
    "y_status_vf = np.asarray(y_status_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time : 1353.243168592453 s\n",
      "Run time : 22.554052809874218 m\n",
      "Run time : 0.3759008801645703 h\n"
     ]
    }
   ],
   "source": [
    "## Name a folder for the outputs to go into\n",
    "outdir = \"output_data_update_19_02/\"\n",
    "build_folder(outdir, False)\n",
    "\n",
    "savedir = (outdir+\"Paper_Results/Trian_Val_No_Ifakara/\")\n",
    "build_folder(savedir, True)\n",
    "\n",
    "val_results = {'loss':[], 'age_group_loss':[], 'species_loss':[], 'age_group_acc':[], 'species_acc':[]}\n",
    "histories = []\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "## Input CNN Size\n",
    "input_layer_dim = len(X[0])\n",
    "\n",
    "## Transform Data\n",
    "y_age_groups_list = [[age] for age in y_age_groups]\n",
    "y_species_list = [[species] for species in y_species]\n",
    "age_groups = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list))\n",
    "age_group_classes = [\"1-4\", \"5-10\", \"11-17\"]\n",
    "species = MultiLabelBinarizer().fit_transform(np.array(y_species_list))\n",
    "species_classes = list(np.unique(y_species_list))\n",
    "y_age_groups_list_vf = [[age] for age in y_age_groups_vf]\n",
    "y_species_list_vf = [[species] for species in y_species_vf]\n",
    "age_groups_vf = MultiLabelBinarizer().fit_transform(np.array(y_age_groups_list_vf))\n",
    "species_vf = MultiLabelBinarizer().fit_transform(np.array(y_species_list_vf))\n",
    "\n",
    "## Labels\n",
    "labels_default, classes_default, outputs_default = [age_groups], [age_group_classes], ['xAgeGroup']\n",
    "labels_default_vf, classes_default_vf, outputs_default_vf = [age_groups_vf], [age_group_classes], ['xAgeGroup']\n",
    "\n",
    "## Declare and train the model\n",
    "model_size = [{'type':'c', 'filter':16, 'kernel':8, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':8, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':3, 'stride':1, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':6, 'stride':2, 'pooling':1},\n",
    "             {'type':'c', 'filter':16, 'kernel':5, 'stride':1, 'pooling':2},\n",
    "             {'type':'d', 'width':500}]\n",
    "\n",
    "## Name the model\n",
    "model_name = ('Valid_Inc_No_Ifakara')\n",
    "\n",
    "## Scale train, test\n",
    "scl = StandardScaler()\n",
    "features_scl = scl.fit(X=np.vstack((X, X_vf)))\n",
    "X_train = features_scl.transform(X=X)\n",
    "X_test = features_scl.transform(X=X_vf)\n",
    "\n",
    "## Split data into test and train\n",
    "y_train, y_test = list(map(lambda y:y, labels_default)), list(map(lambda y:y, labels_default_vf))\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "model_to_test = {\n",
    "    \"model_shape\" : [model_size], # defines the hidden layers of the model\n",
    "    \"model_name\"  : [model_name],\n",
    "    \"input_layer_dim\"  : [input_layer_dim], # size of input layer\n",
    "    \"model_ver_num\"  : [0],\n",
    "    \"fold\"  : [0], # kf.split number on\n",
    "    \"labels\"   : [y_train],\n",
    "    \"features\" : [X_train],\n",
    "    \"classes\"  : [classes_default],\n",
    "    \"outputs\"   : [outputs_default],\n",
    "    \"compile_loss\": [{'age': 'categorical_crossentropy'}],\n",
    "    \"compile_metrics\" :[{'age': 'accuracy'}]\n",
    "}\n",
    "\n",
    "## Call function to train all the models from the dictionary\n",
    "model, history = train_models(model_to_test, savedir)\n",
    "histories.append(history)\n",
    "\n",
    "predicted_labels = list([] for i in range(len(y_train)))\n",
    "true_labels = list([] for i in range(len(y_train)))\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted = [y_predicted]\n",
    "# temp_eval = model.evaluate(X_test, y_test)\n",
    "# for metric, res in zip(model.metrics_names, temp_eval):\n",
    "#     val_results[metric].append(res)\n",
    "# print(val_results)\n",
    "\n",
    "predicted_labels = [x+[y] for x,y in zip(predicted_labels,y_predicted)]\n",
    "true_labels = [x+[y] for x,y in zip(true_labels,y_test)]\n",
    "\n",
    "predicted_labels = [predicted_labels[i][0].tolist() for i in range(len(predicted_labels))]\n",
    "true_labels = [true_labels[i][0].tolist() for i in range(len(true_labels))]\n",
    "\n",
    "## Visualize the results\n",
    "visualize(histories, savedir, model_name, \"0\", classes_default, outputs_default, predicted_labels, true_labels, title='Prediction on Ifakara Validation Field')\n",
    "\n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "\n",
    "end_time = time()\n",
    "print('Run time : {} s'.format(end_time-start_time))\n",
    "print('Run time : {} m'.format((end_time-start_time)/60))\n",
    "print('Run time : {} h'.format((end_time-start_time)/3600))\n",
    "\n",
    "save_obj(history.history, 'Validation_Results_History', savedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepearing csv file with confusion matricies for power analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98365527 0.         0.01634473 0.         0.0483871  0.9516129\n",
      "  0.82590612 0.00713012 0.16696376]\n",
      " [0.72435395 0.         0.27564605 0.         0.11440678 0.88559322\n",
      "  0.52309613 0.00873908 0.46816479]\n",
      " [0.77069536 0.         0.22930464 0.         0.125      0.875\n",
      "  0.42800789 0.01314924 0.55884287]\n",
      " [0.81474978 0.         0.18525022 0.         0.24056604 0.75943396\n",
      "  0.41527778 0.01875    0.56597222]\n",
      " [0.78317757 0.         0.21682243 0.         0.30456853 0.69543147\n",
      "  0.33480826 0.01327434 0.6519174 ]\n",
      " [0.83316683 0.         0.16683317 0.         0.3655914  0.6344086\n",
      "  0.33176471 0.01098039 0.6572549 ]\n",
      " [0.87982833 0.         0.12017167 0.         0.30898876 0.69101124\n",
      "  0.30653266 0.00586265 0.68760469]\n",
      " [0.85863268 0.         0.14136732 0.         0.31176471 0.68823529\n",
      "  0.24079066 0.00808625 0.75112309]\n",
      " [0.84130982 0.         0.15869018 0.         0.50625    0.49375\n",
      "  0.20991254 0.00874636 0.78134111]\n",
      " [0.88965517 0.         0.11034483 0.         0.61184211 0.38815789\n",
      "  0.19409283 0.00738397 0.79852321]\n",
      " [0.87628866 0.         0.12371134 0.         0.48611111 0.51388889\n",
      "  0.14752252 0.         0.85247748]]\n"
     ]
    }
   ],
   "source": [
    "# cms = []\n",
    "# folders = ['valid_data_inc_0', 'valid_data_inc_0.05', 'valid_data_inc_0.1', 'valid_data_inc_0.15', 'valid_data_inc_0.2', 'valid_data_inc_0.25', 'valid_data_inc_0.3', 'valid_data_inc_0.35', 'valid_data_inc_0.4', 'valid_data_inc_0.45', 'valid_data_inc_0.5']\n",
    "# for folder in folders:\n",
    "#     cm = np.load(('Results/Trian_Val_Inc_V2/'+folder+'/Confusion_Matrix_data.npy'))\n",
    "#     cms.append(cm.reshape(-1))\n",
    "# print(np.array(cms))\n",
    "# np.savetxt('Results/Trian_Val_Inc_V2/Confusion_Matricies.csv', cms, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cms = [[0.24, 0.13, 0.63, 0.26, 0.16, 0.58, 0.31, 0.09, 0.60],\n",
    "          [0.40, 0.12, 0.48, 0.21, 0.33, 0.46, 0.20, 0.15, 0.65],\n",
    "          [0.49, 0.09, 0.42, 0.11, 0.40, 0.50, 0.07, 0.14, 0.79],\n",
    "          [0.56, 0.08, 0.36, 0.09, 0.53, 0.38, 0.07, 0.18, 0.75],\n",
    "          [0.57, 0.12, 0.31, 0.09, 0.61, 0.30, 0.09, 0.18, 0.73],\n",
    "          [0.55, 0.07, 0.38, 0.08, 0.62, 0.30, 0.08, 0.13, 0.78],\n",
    "          [0.52, 0.08, 0.40, 0.07, 0.67, 0.26, 0.04, 0.15, 0.81],\n",
    "          [0.51, 0.07, 0.41, 0.06, 0.65, 0.29, 0.04, 0.07, 0.89],\n",
    "          [0.63, 0.08, 0.29, 0.05, 0.72, 0.23, 0.05, 0.10, 0.85],\n",
    "          [0.59, 0.06, 0.34, 0.05, 0.77, 0.19, 0.03, 0.10, 0.86],\n",
    "          [0.57, 0.04, 0.39, 0.05, 0.76, 0.19, 0.03, 0.08, 0.89]]\n",
    "\n",
    "age_cms = np.array(age_cms)\n",
    "np.savetxt('Results/Trian_Val_Inc_V2/Confusion_Matricies.csv', age_cms, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
